# # üî• Sistema Inteligente de Monitoramento e Predi√ß√£o de Queimadas no Pantanal
# ## Aplica√ß√µes em Aprendizado de M√°quina - Ci√™ncia de Dados
# 
# ---
# 
# ### üìã Sum√°rio Executivo
# 
# **Contexto:** O Pantanal, maior plan√≠cie alag√°vel do mundo e patrim√¥nio natural da humanidade, enfrentou em 2020 uma das piores temporadas de queimadas de sua hist√≥ria. Este projeto desenvolve um sistema inteligente de an√°lise e predi√ß√£o utilizando dados geoespaciais reais de focos de calor.
# 
# **Objetivos:**
# - Analisar padr√µes espa√ßo-temporais de queimadas no Pantanal em 2020
# - Identificar clusters naturais de focos com caracter√≠sticas similares
# - Desenvolver modelos preditivos para antecipa√ß√£o de ocorr√™ncias
# - Gerar insights acion√°veis para pol√≠ticas de preven√ß√£o e combate
# 
# **Metodologia:**
# - An√°lise Explorat√≥ria de Dados (EDA)
# - Aprendizado N√£o Supervisionado (K-Means, DBSCAN)
# - Aprendizado Supervisionado (Random Forest, XGBoost)
# - Visualiza√ß√£o Geoespacial Avan√ßada
# 
# ---

# ## 1Ô∏è‚É£ Configura√ß√£o do Ambiente e Importa√ß√£o de Bibliotecas

# ### 1.1 Instala√ß√£o de Depend√™ncias

# Instalar bibliotecas necess√°rias (executar apenas se n√£o estiverem instaladas)
#!pip install -q geopandas folium plotly xgboost scikit-learn pandas numpy matplotlib seaborn

# ### 1.2 Importa√ß√£o de Bibliotecas

# Manipula√ß√£o e an√°lise de dados
# ------------------------------
# 1Ô∏è‚É£ Importa√ß√£o de Bibliotecas
# ------------------------------

# Manipula√ß√£o e an√°lise de dados
import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Banco de dados anal√≠tico em disco/mem√≥ria (alta performance local)
import duckdb

# Visualiza√ß√£o
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# An√°lise geoespacial (sem GeoPandas para evitar GDAL)
# Se voc√™ tiver GeoPandas instalado, pode descomentar a linha abaixo.
# import geopandas as gpd

import folium
from folium.plugins import HeatMap, MarkerCluster

# Machine Learning - Pr√©-processamento
from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.impute import SimpleImputer

# Machine Learning - Algoritmos N√£o Supervisionados
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# Machine Learning - Algoritmos Supervisionados
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.tree import DecisionTreeClassifier
import xgboost as xgb

# Machine Learning - M√©tricas
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score,
    precision_score, recall_score, f1_score, roc_auc_score, roc_curve,
    mean_squared_error, mean_absolute_error, r2_score
)

# (opcional) apenas para printar vers√£o
import sklearn

# (opcional) Spark + Sedona para Big Data
USE_SPARK = False
try:
    from pyspark.sql import SparkSession
    from sedona.register import SedonaRegistrator
    from sedona.utils import SedonaKryoRegistrator, KryoSerializer

    spark = (
        SparkSession.builder
        .appName("QueimadasPantanal2020_2024")
        .config("spark.serializer", KryoSerializer.getName)
        .config("spark.kryo.registrator", SedonaKryoRegistrator.getName)
        .getOrCreate()
    )
    SedonaRegistrator.registerAll(spark)
    USE_SPARK = True
    print("‚úÖ Spark + Sedona inicializados (modo Big Data pronto).")
except ImportError:
    print("‚ö†Ô∏è Spark/Sedona n√£o encontrados. Seguindo apenas com DuckDB + Pandas.")

# Configura√ß√µes de visualiza√ß√£o
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)

print("‚úÖ Todas as bibliotecas importadas com sucesso!")
print(f"üìä Vers√µes principais:")
print(f"   - Pandas: {pd.__version__}")
print(f"   - NumPy: {np.__version__}")
print(f"   - Scikit-learn: {sklearn.__version__}")

# ---

# ## 2Ô∏è‚É£ Etapa 1: Carregamento e Explora√ß√£o Inicial do Dataset
# ------------------------------
# 1Ô∏è‚É£ Importa√ß√£o de Bibliotecas
# ------------------------------

# Manipula√ß√£o e an√°lise de dados
import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Banco de dados anal√≠tico em disco/mem√≥ria (alta performance local)
import duckdb

# Visualiza√ß√£o
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# An√°lise geoespacial (sem GeoPandas para evitar GDAL)
# Se voc√™ tiver GeoPandas instalado, pode descomentar a linha abaixo.
# import geopandas as gpd

import folium
from folium.plugins import HeatMap, MarkerCluster

# Machine Learning - Pr√©-processamento
from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.impute import SimpleImputer

# Machine Learning - Algoritmos N√£o Supervisionados
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# Machine Learning - Algoritmos Supervisionados
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.tree import DecisionTreeClassifier
import xgboost as xgb

# Machine Learning - M√©tricas
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score,
    precision_score, recall_score, f1_score, roc_auc_score, roc_curve,
    mean_squared_error, mean_absolute_error, r2_score
)

# (opcional) apenas para printar vers√£o
import sklearn

# (opcional) Spark + Sedona para Big Data
USE_SPARK = False
try:
    from pyspark.sql import SparkSession
    from sedona.register import SedonaRegistrator
    from sedona.utils import SedonaKryoRegistrator, KryoSerializer

    spark = (
        SparkSession.builder
        .appName("QueimadasPantanal2020_2024")
        .config("spark.serializer", KryoSerializer.getName)
        .config("spark.kryo.registrator", SedonaKryoRegistrator.getName)
        .getOrCreate()
    )
    SedonaRegistrator.registerAll(spark)
    USE_SPARK = True
    print("‚úÖ Spark + Sedona inicializados (modo Big Data pronto).")
except ImportError:
    print("‚ö†Ô∏è Spark/Sedona n√£o encontrados. Seguindo apenas com DuckDB + Pandas.")

# Configura√ß√µes de visualiza√ß√£o
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)

print("‚úÖ Todas as bibliotecas importadas com sucesso!")
print(f"üìä Vers√µes principais:")
print(f"   - Pandas: {pd.__version__}")
print(f"   - NumPy: {np.__version__}")
print(f"   - Scikit-learn: {sklearn.__version__}")

# Criar c√≥pia para trabalho (preservar original)
df = df_original.copy()


# ------------------------------------------------------------------
# (Opcional) Carregar os mesmos dados via Spark/Sedona a partir do Parquet
# ------------------------------------------------------------------
if USE_SPARK:
    print("\n" + "=" * 80)
    print("üî• CARREGANDO DADOS NO SPARK A PARTIR DO PARQUET")
    print("=" * 80)

    df_spark = spark.read.parquet(parquet_path)
    df_spark.createOrReplaceTempView("queimadas_spark")

    print(f"‚úÖ DataFrame Spark carregado: {df_spark.count():,} linhas")
    print("üìå Schema Spark:")
    df_spark.printSchema()

    # Exemplo de consulta em Spark (Big Data)
    exemplo = spark.sql("""
        SELECT ano_dado, COUNT(*) AS focos
        FROM queimadas_spark
        GROUP BY ano_dado
        ORDER BY ano_dado
    """)
    exemplo.show()

    # Se voc√™ quiser jogar uma amostra para Pandas:
    # df_amostra = df_spark.sample(fraction=0.1, seed=42).toPandas()

# ### 2.2 Inspe√ß√£o Inicial da Estrutura

print("=" * 80)
print("üìã INFORMA√á√ïES ESTRUTURAIS DO DATASET")
print("=" * 80)
df.info()

# ### 2.3 Primeiras Linhas do Dataset

print("\n" + "=" * 80)
print("üëÄ PRIMEIRAS 10 LINHAS DO DATASET")
print("=" * 80)
display(df.head(10))

# ### 2.4 Estat√≠sticas Descritivas

print("\n" + "=" * 80)
print("üìä ESTAT√çSTICAS DESCRITIVAS - VARI√ÅVEIS NUM√âRICAS")
print("=" * 80)
display(df.describe().T)

# ### 2.5 An√°lise de Valores Ausentes

print("\n" + "=" * 80)
print("üîç AN√ÅLISE DE VALORES AUSENTES")
print("=" * 80)

missing_data = pd.DataFrame({
    'Coluna': df.columns,
    'Valores_Ausentes': df.isnull().sum(),
    'Percentual (%)': (df.isnull().sum() / len(df)) * 100
})
missing_data = missing_data[missing_data['Valores_Ausentes'] > 0].sort_values('Valores_Ausentes', ascending=False)

if len(missing_data) > 0:
    display(missing_data)
    
    # Visualizar valores ausentes
    plt.figure(figsize=(12, 6))
    sns.barplot(data=missing_data, x='Coluna', y='Percentual (%)', palette='Reds_r')
    plt.title('Percentual de Valores Ausentes por Vari√°vel', fontsize=14, fontweight='bold')
    plt.xlabel('Vari√°veis')
    plt.ylabel('Percentual de Dados Ausentes (%)')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()
else:
    print("‚úÖ Nenhum valor ausente detectado no dataset!")

# ### 2.6 An√°lise de Duplicatas

duplicatas = df.duplicated().sum()
print(f"\nüîç Registros duplicados encontrados: {duplicatas:,}")

if duplicatas > 0:
    print(f"‚ö†Ô∏è  Percentual de duplicatas: {(duplicatas/len(df)*100):.2f}%")
    print("   ‚Üí Ser√° necess√°rio tratamento na etapa de pr√©-processamento")
else:
    print("‚úÖ Nenhuma duplicata encontrada!")

# ### 2.7 Identifica√ß√£o de Tipos de Vari√°veis

print("\n" + "=" * 80)
print("üè∑Ô∏è  CLASSIFICA√á√ÉO DAS VARI√ÅVEIS")
print("=" * 80)

# Identificar colunas por tipo
colunas_numericas = df.select_dtypes(include=[np.number]).columns.tolist()
colunas_categoricas = df.select_dtypes(include=['object']).columns.tolist()
colunas_datetime = [col for col in df.columns if 'data' in col.lower() or 'date' in col.lower()]

print(f"\nüìä Vari√°veis Num√©ricas ({len(colunas_numericas)}):")
for col in colunas_numericas:
    print(f"   ‚Ä¢ {col}")

print(f"\nüè∑Ô∏è  Vari√°veis Categ√≥ricas ({len(colunas_categoricas)}):")
for col in colunas_categoricas:
    print(f"   ‚Ä¢ {col} - {df[col].nunique()} valores √∫nicos")

print(f"\nüìÖ Poss√≠veis Vari√°veis Temporais:")
for col in colunas_datetime:
    print(f"   ‚Ä¢ {col}")

# ---

# ## 3Ô∏è‚É£ Etapa 2: An√°lise Explorat√≥ria de Dados Aprofundada

print("\n" + "=" * 80)
print("üî¨ INICIANDO AN√ÅLISE EXPLORAT√ìRIA APROFUNDADA")
print("=" * 80)

# ### 3.1 An√°lise Temporal

# Identificar e converter colunas de data
if 'data_hora_gmt' in df.columns or 'datahora' in df.columns:
    col_data = 'data_hora_gmt' if 'data_hora_gmt' in df.columns else 'datahora'
    df[col_data] = pd.to_datetime(df[col_data], errors='coerce')
    
    # Extrair componentes temporais
    df['data'] = df[col_data].dt.date
    df['ano'] = df[col_data].dt.year
    df['mes'] = df[col_data].dt.month
    df['dia'] = df[col_data].dt.day
    df['dia_semana'] = df[col_data].dt.dayofweek
    df['nome_dia_semana'] = df[col_data].dt.day_name()
    df['nome_mes'] = df[col_data].dt.month_name()
    df['dia_do_ano'] = df[col_data].dt.dayofyear
    df['semana_do_ano'] = df[col_data].dt.isocalendar().week
    
    # Definir esta√ß√µes do ano (Hemisf√©rio Sul)
    def definir_estacao(mes):
        if mes in [12, 1, 2]:
            return 'Ver√£o'
        elif mes in [3, 4, 5]:
            return 'Outono'
        elif mes in [6, 7, 8]:
            return 'Inverno'
        else:
            return 'Primavera'
    
    df['estacao'] = df['mes'].apply(definir_estacao)
    
    print(f"‚úÖ An√°lise temporal configurada para: {col_data}")
    print(f"   Per√≠odo: {df['data'].min()} at√© {df['data'].max()}")

# ### 3.2 S√©ries Temporais de Ocorr√™ncias

# Ocorr√™ncias di√°rias
ocorrencias_diarias = df.groupby('data').size().reset_index(name='ocorrencias')
ocorrencias_diarias['data'] = pd.to_datetime(ocorrencias_diarias['data'])

# Visualiza√ß√£o de s√©rie temporal
fig = make_subplots(
    rows=2, cols=1,
    subplot_titles=('S√©rie Temporal de Focos de Queimadas - 2020 (Di√°rio)',
                   'Tend√™ncia Semanal (M√©dia M√≥vel 7 dias)'),
    vertical_spacing=0.12
)

# Gr√°fico di√°rio
fig.add_trace(
    go.Scatter(x=ocorrencias_diarias['data'], y=ocorrencias_diarias['ocorrencias'],
               mode='lines', name='Ocorr√™ncias Di√°rias',
               line=dict(color='orangered', width=1)),
    row=1, col=1
)

# M√©dia m√≥vel de 7 dias
ocorrencias_diarias['media_movel_7d'] = ocorrencias_diarias['ocorrencias'].rolling(window=7).mean()
fig.add_trace(
    go.Scatter(x=ocorrencias_diarias['data'], y=ocorrencias_diarias['media_movel_7d'],
               mode='lines', name='M√©dia M√≥vel (7 dias)',
               line=dict(color='darkred', width=3)),
    row=2, col=1
)

fig.update_xaxes(title_text="Data", row=2, col=1)
fig.update_yaxes(title_text="N√∫mero de Focos", row=1, col=1)
fig.update_yaxes(title_text="N√∫mero de Focos", row=2, col=1)

fig.update_layout(height=700, title_text="üìà An√°lise Temporal de Queimadas no Pantanal - 2020",
                  showlegend=True, hovermode='x unified')
fig.show()

# Estat√≠sticas temporais
print("\nüìä ESTAT√çSTICAS TEMPORAIS:")
print(f"   ‚Ä¢ Total de focos detectados: {len(df):,}")
print(f"   ‚Ä¢ M√©dia di√°ria: {ocorrencias_diarias['ocorrencias'].mean():.1f} focos/dia")
print(f"   ‚Ä¢ Mediana di√°ria: {ocorrencias_diarias['ocorrencias'].median():.1f} focos/dia")
print(f"   ‚Ä¢ Dia com mais focos: {ocorrencias_diarias.loc[ocorrencias_diarias['ocorrencias'].idxmax(), 'data']} ({ocorrencias_diarias['ocorrencias'].max():,} focos)")
print(f"   ‚Ä¢ Dia com menos focos: {ocorrencias_diarias.loc[ocorrencias_diarias['ocorrencias'].idxmin(), 'data']} ({ocorrencias_diarias['ocorrencias'].min():,} focos)")

# ### 3.3 An√°lise por M√™s e Esta√ß√£o

# Agregar por m√™s
ocorrencias_mensais = df.groupby(['mes', 'nome_mes']).size().reset_index(name='ocorrencias')
ocorrencias_mensais = ocorrencias_mensais.sort_values('mes')

# Agregar por esta√ß√£o
ocorrencias_estacao = df.groupby('estacao').size().reset_index(name='ocorrencias')

# Visualiza√ß√£o
fig = make_subplots(
    rows=1, cols=2,
    subplot_titles=('Ocorr√™ncias por M√™s', 'Ocorr√™ncias por Esta√ß√£o do Ano'),
    specs=[[{'type': 'bar'}, {'type': 'pie'}]]
)

# Gr√°fico mensal
fig.add_trace(
    go.Bar(x=ocorrencias_mensais['nome_mes'], y=ocorrencias_mensais['ocorrencias'],
           marker_color='orangered', name='Mensal'),
    row=1, col=1
)

# Gr√°fico por esta√ß√£o
fig.add_trace(
    go.Pie(labels=ocorrencias_estacao['estacao'], values=ocorrencias_estacao['ocorrencias'],
           hole=0.4, marker_colors=['#FFD700', '#FF8C00', '#8B4513', '#90EE90']),
    row=1, col=2
)

fig.update_xaxes(title_text="M√™s", row=1, col=1, tickangle=-45)
fig.update_yaxes(title_text="N√∫mero de Focos", row=1, col=1)

fig.update_layout(height=500, title_text="üìÖ Distribui√ß√£o Temporal das Queimadas",
                  showlegend=False)
fig.show()

# Ranking de meses
print("\nüî• RANKING DE MESES MAIS CR√çTICOS:")
top_meses = ocorrencias_mensais.sort_values('ocorrencias', ascending=False)
for idx, row in top_meses.iterrows():
    percentual = (row['ocorrencias'] / len(df)) * 100
    print(f"   {row['mes']:2d}. {row['nome_mes']:10s} - {row['ocorrencias']:7,} focos ({percentual:5.2f}%)")

# ### 3.4 An√°lise Espacial - Coordenadas Geogr√°ficas

# Verificar presen√ßa de coordenadas
if 'latitude' in df.columns and 'longitude' in df.columns:
    
    print("\nüó∫Ô∏è  AN√ÅLISE GEOESPACIAL:")
    print(f"   ‚Ä¢ Latitude - M√≠n: {df['latitude'].min():.4f}, M√°x: {df['latitude'].max():.4f}")
    print(f"   ‚Ä¢ Longitude - M√≠n: {df['longitude'].min():.4f}, M√°x: {df['longitude'].max():.4f}")
    
    # Remover coordenadas inv√°lidas
    df_geo = df[(df['latitude'].notna()) & (df['longitude'].notna())]
    df_geo = df_geo[(df_geo['latitude'] >= -90) & (df_geo['latitude'] <= 90)]
    df_geo = df_geo[(df_geo['longitude'] >= -180) & (df_geo['longitude'] <= 180)]
    
    print(f"   ‚Ä¢ Registros v√°lidos para an√°lise espacial: {len(df_geo):,} ({len(df_geo)/len(df)*100:.2f}%)")
    
    # Scatter plot das coordenadas
    fig = px.scatter(df_geo.sample(min(50000, len(df_geo))),  # Amostra para performance
                     x='longitude', y='latitude',
                     color='mes', size_max=5,
                     title='üó∫Ô∏è Distribui√ß√£o Espacial dos Focos de Queimadas no Pantanal',
                     labels={'longitude': 'Longitude', 'latitude': 'Latitude', 'mes': 'M√™s'},
                     color_continuous_scale='YlOrRd',
                     height=600)
    
    fig.update_traces(marker=dict(size=3, opacity=0.6))
    fig.update_layout(xaxis_title="Longitude", yaxis_title="Latitude")
    fig.show()

# ### 3.5 An√°lise por Estado e Munic√≠pio

if 'estado' in df.columns:
    # An√°lise por estado
    ocorrencias_estado = df.groupby('estado').size().reset_index(name='ocorrencias')
    ocorrencias_estado = ocorrencias_estado.sort_values('ocorrencias', ascending=False)
    
    print("\nüèõÔ∏è  RANKING DE ESTADOS MAIS AFETADOS:")
    for idx, row in ocorrencias_estado.head(10).iterrows():
        percentual = (row['ocorrencias'] / len(df)) * 100
        print(f"   ‚Ä¢ {row['estado']:20s} - {row['ocorrencias']:7,} focos ({percentual:5.2f}%)")
    
    # Visualiza√ß√£o
    fig = px.bar(ocorrencias_estado.head(15), x='estado', y='ocorrencias',
                 title='üèõÔ∏è Estados com Maior N√∫mero de Focos de Queimadas',
                 labels={'estado': 'Estado', 'ocorrencias': 'N√∫mero de Focos'},
                 color='ocorrencias', color_continuous_scale='Reds',
                 height=500)
    fig.update_layout(xaxis_tickangle=-45)
    fig.show()

if 'municipio' in df.columns:
    # Top munic√≠pios
    ocorrencias_municipio = df.groupby('municipio').size().reset_index(name='ocorrencias')
    ocorrencias_municipio = ocorrencias_municipio.sort_values('ocorrencias', ascending=False)
    
    print("\nüèòÔ∏è  TOP 15 MUNIC√çPIOS MAIS AFETADOS:")
    for idx, row in ocorrencias_municipio.head(15).iterrows():
        percentual = (row['ocorrencias'] / len(df)) * 100
        print(f"   ‚Ä¢ {row['municipio']:30s} - {row['ocorrencias']:6,} focos ({percentual:5.2f}%)")

# ### 3.6 An√°lise de Intensidade (FRP - Fire Radiative Power)

if 'frp' in df.columns or 'potencia_fogo' in df.columns:
    col_frp = 'frp' if 'frp' in df.columns else 'potencia_fogo'
    
    print(f"\nüî• AN√ÅLISE DE INTENSIDADE DO FOGO ({col_frp}):")
    print(f"   ‚Ä¢ M√©dia: {df[col_frp].mean():.2f} MW")
    print(f"   ‚Ä¢ Mediana: {df[col_frp].median():.2f} MW")
    print(f"   ‚Ä¢ Desvio Padr√£o: {df[col_frp].std():.2f} MW")
    print(f"   ‚Ä¢ M√≠nimo: {df[col_frp].min():.2f} MW")
    print(f"   ‚Ä¢ M√°ximo: {df[col_frp].max():.2f} MW")
    
    # Distribui√ß√£o do FRP
    fig = make_subplots(rows=1, cols=2,
                        subplot_titles=('Distribui√ß√£o da Pot√™ncia Radiativa do Fogo',
                                       'Boxplot - Detec√ß√£o de Outliers'))
    
    fig.add_trace(go.Histogram(x=df[col_frp], nbinsx=50, name='FRP',
                               marker_color='orangered'), row=1, col=1)
    
    fig.add_trace(go.Box(y=df[col_frp], name='FRP',
                         marker_color='orangered', boxmean='sd'), row=1, col=2)
    
    fig.update_xaxes(title_text="Pot√™ncia Radiativa (MW)", row=1, col=1)
    fig.update_yaxes(title_text="Frequ√™ncia", row=1, col=1)
    fig.update_yaxes(title_text="Pot√™ncia Radiativa (MW)", row=1, col=2)
    
    fig.update_layout(height=400, showlegend=False,
                      title_text="üìä An√°lise da Intensidade dos Focos de Queimadas")
    fig.show()

# ### 3.7 Matriz de Correla√ß√£o

# Selecionar apenas colunas num√©ricas relevantes
colunas_correlacao = [col for col in colunas_numericas if col in df.columns]
if len(colunas_correlacao) > 1:
    correlacao = df[colunas_correlacao].corr()
    
    plt.figure(figsize=(12, 10))
    mask = np.triu(np.ones_like(correlacao, dtype=bool))
    sns.heatmap(correlacao, mask=mask, annot=True, fmt='.2f', cmap='RdYlBu_r',
                center=0, square=True, linewidths=1, cbar_kws={"shrink": 0.8})
    plt.title('üîó Matriz de Correla√ß√£o entre Vari√°veis Num√©ricas', 
              fontsize=14, fontweight='bold', pad=20)
    plt.tight_layout()
    plt.show()

print("\n‚úÖ An√°lise Explorat√≥ria de Dados conclu√≠da!")

# ---

# ## 4Ô∏è‚É£ Etapa 3: Pr√©-processamento e Feature Engineering

print("\n" + "=" * 80)
print("‚öôÔ∏è  INICIANDO PR√â-PROCESSAMENTO E ENGENHARIA DE FEATURES")
print("=" * 80)

# ### 4.1 Tratamento de Valores Ausentes

# Criar c√≥pia para pr√©-processamento
df_processed = df.copy()

# Imputa√ß√£o de valores ausentes em vari√°veis num√©ricas
if df_processed[colunas_numericas].isnull().sum().sum() > 0:
    print("\nüîß Tratando valores ausentes em vari√°veis num√©ricas...")
    imputer_num = SimpleImputer(strategy='median')
    df_processed[colunas_numericas] = imputer_num.fit_transform(df_processed[colunas_numericas])
    print("   ‚úÖ Imputa√ß√£o com mediana aplicada")

# Imputa√ß√£o de valores ausentes em vari√°veis categ√≥ricas
cols_cat_com_missing = [col for col in colunas_categoricas if df_processed[col].isnull().sum() > 0]
if len(cols_cat_com_missing) > 0:
    print("\nüîß Tratando valores ausentes em vari√°veis categ√≥ricas...")
    for col in cols_cat_com_missing:
        df_processed[col].fillna('DESCONHECIDO', inplace=True)
    print(f"   ‚úÖ {len(cols_cat_com_missing)} colunas categ√≥ricas tratadas")

# ### 4.2 Tratamento de Outliers

# Identificar outliers usando IQR
def identificar_outliers_iqr(serie):
    Q1 = serie.quantile(0.25)
    Q3 = serie.quantile(0.75)
    IQR = Q3 - Q1
    limite_inferior = Q1 - 1.5 * IQR
    limite_superior = Q3 + 1.5 * IQR
    return (serie < limite_inferior) | (serie > limite_superior)

# An√°lise de outliers nas principais vari√°veis
print("\nüîç AN√ÅLISE DE OUTLIERS:")
if col_frp in df_processed.columns:
    outliers_frp = identificar_outliers_iqr(df_processed[col_frp])
    print(f"   ‚Ä¢ {col_frp}: {outliers_frp.sum():,} outliers ({outliers_frp.sum()/len(df_processed)*100:.2f}%)")
    
    # Para queimadas, valores extremos podem ser reais (inc√™ndios severos)
    # Vamos manter mas criar flag
    df_processed['is_outlier_frp'] = outliers_frp.astype(int)

# ### 4.3 Feature Engineering - Cria√ß√£o de Novas Vari√°veis

print("\nüõ†Ô∏è  ENGENHARIA DE FEATURES:")

# 1. Features temporais j√° criadas: dia_semana, mes, estacao, etc.

# 2. Flag de per√≠odo cr√≠tico (meses de seca - julho a outubro)
df_processed['periodo_critico'] = df_processed['mes'].apply(
    lambda x: 1 if x in [7, 8, 9, 10] else 0
)
print("   ‚úÖ Feature 'periodo_critico' criada (meses de seca)")

# 3. Densidade temporal (focos por dia)
densidade_temporal = df_processed.groupby('data').size().to_dict()
df_processed['densidade_diaria'] = df_processed['data'].map(densidade_temporal)
print("   ‚úÖ Feature 'densidade_diaria' criada")

# 4. Flag de fim de semana
df_processed['fim_de_semana'] = df_processed['dia_semana'].apply(
    lambda x: 1 if x >= 5 else 0
)
print("   ‚úÖ Feature 'fim_de_semana' criada")

# 5. Classifica√ß√£o de intensidade do fogo
if col_frp in df_processed.columns:
    def classificar_intensidade(frp):
        if frp < 10:
            return 'Baixa'
        elif frp < 50:
            return 'M√©dia'
        elif frp < 100:
            return 'Alta'
        else:
            return 'Muito Alta'
    
    df_processed['intensidade_classe'] = df_processed[col_frp].apply(classificar_intensidade)
    print("   ‚úÖ Feature 'intensidade_classe' criada")

# 6. Coordenadas arredondadas para an√°lise de hotspots
if 'latitude' in df_processed.columns and 'longitude' in df_processed.columns:
    df_processed['lat_round'] = df_processed['latitude'].round(2)
    df_processed['lon_round'] = df_processed['longitude'].round(2)
    
    # Criar identificador de grid
    df_processed['grid_id'] = (df_processed['lat_round'].astype(str) + '_' + 
                                df_processed['lon_round'].astype(str))
    
    # Densidade espacial
    densidade_espacial = df_processed.groupby('grid_id').size().to_dict()
    df_processed['densidade_espacial'] = df_processed['grid_id'].map(densidade_espacial)
    print("   ‚úÖ Features geoespaciais criadas (grid, densidade espacial)")

print(f"\nüìä Dataset ap√≥s feature engineering: {df_processed.shape}")

# ### 4.4 Codifica√ß√£o de Vari√°veis Categ√≥ricas

# Label Encoding para vari√°veis categ√≥ricas ordinais
label_encoders = {}

if 'intensidade_classe' in df_processed.columns:
    le_intensidade = LabelEncoder()
    df_processed['intensidade_classe_encoded'] = le_intensidade.fit_transform(
        df_processed['intensidade_classe']
    )
    label_encoders['intensidade_classe'] = le_intensidade
    print("\n‚úÖ Label Encoding aplicado em 'intensidade_classe'")

# Para estado e munic√≠pio, usaremos frequ√™ncia (j√° que h√° muitas categorias)
if 'estado' in df_processed.columns:
    estado_freq = df_processed['estado'].value_counts(normalize=True).to_dict()
    df_processed['estado_freq'] = df_processed['estado'].map(estado_freq)
    print("‚úÖ Frequency encoding aplicado em 'estado'")

if 'municipio' in df_processed.columns:
    municipio_freq = df_processed['municipio'].value_counts(normalize=True).to_dict()
    df_processed['municipio_freq'] = df_processed['municipio'].map(municipio_freq)
    print("‚úÖ Frequency encoding aplicado em 'municipio'")

# ### 4.5 Normaliza√ß√£o de Features

# Selecionar features num√©ricas para normaliza√ß√£o
features_para_normalizar = ['latitude', 'longitude', 'densidade_diaria', 
                            'densidade_espacial']
features_para_normalizar = [f for f in features_para_normalizar if f in df_processed.columns]

if col_frp in df_processed.columns:
    features_para_normalizar.append(col_frp)

# Aplicar MinMaxScaler
scaler = MinMaxScaler()
df_processed[features_para_normalizar] = scaler.fit_transform(
    df_processed[features_para_normalizar]
)

print(f"\n‚úÖ Normaliza√ß√£o aplicada em {len(features_para_normalizar)} features")
print(f"   Features normalizadas: {', '.join(features_para_normalizar)}")

print("\n‚úÖ Pr√©-processamento conclu√≠do!")

# ---

# ## 5Ô∏è‚É£ Etapa 4: An√°lise de Clusteriza√ß√£o (Aprendizado N√£o Supervisionado)

print("\n" + "=" * 80)
print("üî¨ INICIANDO AN√ÅLISE DE CLUSTERIZA√á√ÉO")
print("=" * 80)

# ### 5.1 Prepara√ß√£o dos Dados para Clustering

# Selecionar features para clustering
features_clustering = ['latitude', 'longitude', 'mes', 'dia_do_ano']

if col_frp in df_processed.columns:
    features_clustering.append(col_frp)
if 'densidade_espacial' in df_processed.columns:
    features_clustering.append('densidade_espacial')
if 'densidade_diaria' in df_processed.columns:
    features_clustering.append('densidade_diaria')

# Criar dataset para clustering (remover NaNs)
df_clustering = df_processed[features_clustering].dropna()

# Padronizar para clustering
scaler_clustering = StandardScaler()
X_clustering = scaler_clustering.fit_transform(df_clustering)

print(f"üìä Dataset para clustering:")
print(f"   ‚Ä¢ Amostras: {X_clustering.shape[0]:,}")
print(f"   ‚Ä¢ Features: {X_clustering.shape[1]}")
print(f"   ‚Ä¢ Features utilizadas: {', '.join(features_clustering)}")

# ### 5.2 M√©todo do Cotovelo (Elbow Method)

print("\nüîç Determinando n√∫mero √≥timo de clusters...")

# Testar diferentes n√∫meros de clusters
K_range = range(2, 11)
inertias = []
silhouette_scores = []

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    
    # Usar amostra para acelerar processamento
    sample_size = min(50000, len(X_clustering))
    sample_indices = np.random.choice(len(X_clustering), sample_size, replace=False)
    X_sample = X_clustering[sample_indices]
    
    kmeans.fit(X_sample)
    inertias.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X_sample, kmeans.labels_))
    
    print(f"   K={k}: In√©rcia={kmeans.inertia_:.2f}, Silhueta={silhouette_scores[-1]:.3f}")

# Visualizar m√©todo do cotovelo e silhueta
fig = make_subplots(rows=1, cols=2,
                    subplot_titles=('M√©todo do Cotovelo', 'Coeficiente de Silhueta'))

fig.add_trace(go.Scatter(x=list(K_range), y=inertias, mode='lines+markers',
                         line=dict(color='orangered', width=3),
                         marker=dict(size=10)), row=1, col=1)

fig.add_trace(go.Scatter(x=list(K_range), y=silhouette_scores, mode='lines+markers',
                         line=dict(color='green', width=3),
                         marker=dict(size=10)), row=1, col=2)

fig.update_xaxes(title_text="N√∫mero de Clusters (K)", row=1, col=1)
fig.update_xaxes(title_text="N√∫mero de Clusters (K)", row=1, col=2)
fig.update_yaxes(title_text="In√©rcia (WCSS)", row=1, col=1)
fig.update_yaxes(title_text="Coeficiente de Silhueta", row=1, col=2)

fig.update_layout(height=400, title_text="üìä Determina√ß√£o do N√∫mero √ìtimo de Clusters",
                  showlegend=False)
fig.show()

# ### 5.3 Aplica√ß√£o do K-Means com K √ìtimo

# Determinar K √≥timo (maior silhueta)
k_otimo = K_range[np.argmax(silhouette_scores)]
print(f"\nüéØ N√∫mero √≥timo de clusters selecionado: K = {k_otimo}")
print(f"   (baseado no maior coeficiente de silhueta: {max(silhouette_scores):.3f})")

# Treinar modelo final com todos os dados
kmeans_final = KMeans(n_clusters=k_otimo, random_state=42, n_init=20)

# Usar amostra estratificada para treino
sample_size_final = min(100000, len(X_clustering))
sample_indices_final = np.random.choice(len(X_clustering), sample_size_final, replace=False)
X_sample_final = X_clustering[sample_indices_final]

print(f"\n‚è≥ Treinando K-Means com {sample_size_final:,} amostras...")
kmeans_final.fit(X_sample_final)

# Predizer clusters para todas as amostras
print("‚è≥ Atribuindo clusters a todas as observa√ß√µes...")
clusters = kmeans_final.predict(X_clustering)
df_clustering['cluster'] = clusters

# M√©tricas de qualidade
silhouette_avg = silhouette_score(X_sample_final, kmeans_final.predict(X_sample_final))
davies_bouldin = davies_bouldin_score(X_sample_final, kmeans_final.predict(X_sample_final))
calinski_harabasz = calinski_harabasz_score(X_sample_final, kmeans_final.predict(X_sample_final))

print(f"\nüìä M√âTRICAS DE QUALIDADE DO CLUSTERING:")
print(f"   ‚Ä¢ Coeficiente de Silhueta: {silhouette_avg:.3f} (quanto maior, melhor)")
print(f"   ‚Ä¢ √çndice Davies-Bouldin: {davies_bouldin:.3f} (quanto menor, melhor)")
print(f"   ‚Ä¢ √çndice Calinski-Harabasz: {calinski_harabasz:.2f} (quanto maior, melhor)")

# Adicionar clusters ao dataframe processado
df_processed.loc[df_clustering.index, 'cluster'] = clusters

# ### 5.4 An√°lise e Interpreta√ß√£o dos Clusters

print("\n" + "=" * 80)
print("üîç AN√ÅLISE DETALHADA DOS CLUSTERS")
print("=" * 80)

# Estat√≠sticas por cluster
for cluster_id in range(k_otimo):
    cluster_data = df_clustering[df_clustering['cluster'] == cluster_id]
    n_amostras = len(cluster_data)
    percentual = (n_amostras / len(df_clustering)) * 100
    
    print(f"\n{'='*60}")
    print(f"üìç CLUSTER {cluster_id} - {n_amostras:,} focos ({percentual:.2f}%)")
    print(f"{'='*60}")
    
    # Estat√≠sticas das features originais
    for feat in features_clustering:
        if feat in df_processed.columns:
            valores = df_processed.loc[cluster_data.index, feat]
            print(f"   ‚Ä¢ {feat:25s}: Œº={valores.mean():8.2f}, œÉ={valores.std():8.2f}, "
                  f"min={valores.min():8.2f}, max={valores.max():8.2f}")
    
    # Distribui√ß√£o temporal
    if 'mes' in cluster_data.columns:
        mes_predominante = df_processed.loc[cluster_data.index, 'nome_mes'].mode()[0]
        print(f"\n   üóìÔ∏è  M√™s predominante: {mes_predominante}")
    
    # Distribui√ß√£o espacial
    if 'latitude' in cluster_data.columns and 'longitude' in cluster_data.columns:
        lat_centro = df_processed.loc[cluster_data.index, 'latitude'].mean()
        lon_centro = df_processed.loc[cluster_data.index, 'longitude'].mean()
        print(f"   üó∫Ô∏è  Centro geogr√°fico: ({lat_centro:.2f}, {lon_centro:.2f})")
    
    # Intensidade m√©dia
    if col_frp in df_processed.columns:
        frp_medio = df_processed.loc[cluster_data.index, col_frp].mean()
        print(f"   üî• Intensidade m√©dia (FRP): {frp_medio:.2f} MW")

# ### 5.5 Visualiza√ß√£o dos Clusters

# Visualiza√ß√£o geoespacial dos clusters
if 'latitude' in df_clustering.columns and 'longitude' in df_clustering.columns:
    
    # Amostrar para visualiza√ß√£o
    df_viz_sample = df_clustering.sample(min(20000, len(df_clustering)))
    
    # Recuperar coordenadas originais
    lat_original = df_processed.loc[df_viz_sample.index, 'latitude']
    lon_original = df_processed.loc[df_viz_sample.index, 'longitude']
    
    fig = px.scatter_mapbox(
        df_viz_sample,
        lat=lat_original,
        lon=lon_original,
        color='cluster',
        color_continuous_scale='Viridis',
        zoom=4,
        height=600,
        title='üó∫Ô∏è Distribui√ß√£o Espacial dos Clusters de Queimadas',
        labels={'cluster': 'Cluster'}
    )
    
    fig.update_layout(mapbox_style="open-street-map")
    fig.update_traces(marker=dict(size=5, opacity=0.6))
    fig.show()

# Distribui√ß√£o dos clusters ao longo do tempo
cluster_temporal = df_processed[df_processed['cluster'].notna()].groupby(
    ['mes', 'cluster']
).size().reset_index(name='ocorrencias')

fig = px.bar(cluster_temporal, x='mes', y='ocorrencias', color='cluster',
             title='üìä Distribui√ß√£o Temporal dos Clusters',
             labels={'mes': 'M√™s', 'ocorrencias': 'N√∫mero de Focos', 'cluster': 'Cluster'},
             barmode='stack', height=500)
fig.show()

# ### 5.6 DBSCAN - Clustering Baseado em Densidade

print("\n" + "=" * 80)
print("üî¨ APLICANDO DBSCAN (Density-Based Clustering)")
print("=" * 80)

# Usar apenas features espaciais para DBSCAN
features_dbscan = ['latitude', 'longitude']
X_dbscan = df_clustering[features_dbscan].values

# Usar amostra para DBSCAN (√© computacionalmente intensivo)
sample_size_dbscan = min(30000, len(X_dbscan))
sample_indices_dbscan = np.random.choice(len(X_dbscan), sample_size_dbscan, replace=False)
X_dbscan_sample = X_dbscan[sample_indices_dbscan]

print(f"‚è≥ Executando DBSCAN em {sample_size_dbscan:,} amostras...")

# Par√¢metros DBSCAN (ajustar eps baseado na escala dos dados)
dbscan = DBSCAN(eps=0.3, min_samples=50)
clusters_dbscan = dbscan.fit_predict(X_dbscan_sample)

# An√°lise dos resultados
n_clusters_dbscan = len(set(clusters_dbscan)) - (1 if -1 in clusters_dbscan else 0)
n_noise = list(clusters_dbscan).count(-1)

print(f"\nüìä RESULTADOS DO DBSCAN:")
print(f"   ‚Ä¢ N√∫mero de clusters encontrados: {n_clusters_dbscan}")
print(f"   ‚Ä¢ Pontos de ru√≠do (outliers): {n_noise:,} ({n_noise/len(clusters_dbscan)*100:.2f}%)")
print(f"   ‚Ä¢ Pontos em clusters: {sample_size_dbscan - n_noise:,}")

# Visualizar clusters DBSCAN
if n_clusters_dbscan > 0:
    df_dbscan_viz = pd.DataFrame({
        'latitude': X_dbscan_sample[:, 0],
        'longitude': X_dbscan_sample[:, 1],
        'cluster': clusters_dbscan
    })
    
    fig = px.scatter(df_dbscan_viz, x='longitude', y='latitude', color='cluster',
                     title='üó∫Ô∏è DBSCAN: Clusters de Densidade Espacial',
                     labels={'cluster': 'Cluster (-1 = Ru√≠do)'},
                     color_continuous_scale='Viridis',
                     height=600)
    fig.update_traces(marker=dict(size=3, opacity=0.6))
    fig.show()

print("\n‚úÖ An√°lise de Clusteriza√ß√£o conclu√≠da!")

# ---

# ## 6Ô∏è‚É£ Etapa 5: Modelagem Preditiva (Aprendizado Supervisionado)

print("\n" + "=" * 80)
print("ü§ñ INICIANDO MODELAGEM PREDITIVA")
print("=" * 80)

# ### 6.1 Defini√ß√£o do Problema e Prepara√ß√£o dos Dados

# Vamos criar um problema de classifica√ß√£o: predizer se um foco ter√° alta intensidade
# Definir limiar para classifica√ß√£o (baseado no terceiro quartil do FRP)

if col_frp in df_processed.columns:
    
    # Remover outliers extremos para melhor modelagem
    df_ml = df_processed[df_processed[col_frp] <= df_processed[col_frp].quantile(0.95)].copy()
    
    # Definir target: alta intensidade
    limiar_intensidade = df_ml[col_frp].quantile(0.75)
    df_ml['alta_intensidade'] = (df_ml[col_frp] > limiar_intensidade).astype(int)
    
    print(f"üéØ PROBLEMA DE CLASSIFICA√á√ÉO: Predizer Alta Intensidade de Queimadas")
    print(f"   ‚Ä¢ Limiar definido: {limiar_intensidade:.2f} MW (Q3)")
    print(f"   ‚Ä¢ Classe 0 (Baixa/M√©dia): {(df_ml['alta_intensidade']==0).sum():,} amostras "
          f"({(df_ml['alta_intensidade']==0).sum()/len(df_ml)*100:.1f}%)")
    print(f"   ‚Ä¢ Classe 1 (Alta): {(df_ml['alta_intensidade']==1).sum():,} amostras "
          f"({(df_ml['alta_intensidade']==1).sum()/len(df_ml)*100:.1f}%)")
    
    # Selecionar features para modelagem
    features_ml = ['mes', 'dia_do_ano', 'dia_semana', 'periodo_critico', 
                   'fim_de_semana', 'densidade_diaria', 'densidade_espacial']
    
    # Adicionar coordenadas se dispon√≠veis
    if 'latitude' in df_ml.columns and 'longitude' in df_ml.columns:
        features_ml.extend(['latitude', 'longitude'])
    
    # Adicionar features encodadas se dispon√≠veis
    if 'estado_freq' in df_ml.columns:
        features_ml.append('estado_freq')
    if 'municipio_freq' in df_ml.columns:
        features_ml.append('municipio_freq')
    if 'intensidade_classe_encoded' in df_ml.columns:
        features_ml.append('intensidade_classe_encoded')
    
    # Verificar disponibilidade das features
    features_ml = [f for f in features_ml if f in df_ml.columns]
    
    print(f"\nüìä Features selecionadas para modelagem ({len(features_ml)}):")
    for feat in features_ml:
        print(f"   ‚Ä¢ {feat}")
    
    # Preparar X e y
    df_ml_clean = df_ml[features_ml + ['alta_intensidade']].dropna()
    X = df_ml_clean[features_ml]
    y = df_ml_clean['alta_intensidade']
    
    print(f"\nüìä Dataset para modelagem:")
    print(f"   ‚Ä¢ Total de amostras: {len(X):,}")
    print(f"   ‚Ä¢ N√∫mero de features: {X.shape[1]}")
    
    # ### 6.2 Divis√£o em Conjuntos de Treino e Teste
    
    # Usar 20% para teste
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"\n‚úÇÔ∏è  DIVIS√ÉO DOS DADOS:")
    print(f"   ‚Ä¢ Treino: {len(X_train):,} amostras ({len(X_train)/len(X)*100:.1f}%)")
    print(f"   ‚Ä¢ Teste:  {len(X_test):,} amostras ({len(X_test)/len(X)*100:.1f}%)")
    
    # Distribui√ß√£o das classes
    print("\n   Distribui√ß√£o nos conjuntos:")
    print(f"   Treino - Classe 0: {(y_train==0).sum():,} | Classe 1: {(y_train==1).sum():,}")
    print(f"   Teste  - Classe 0: {(y_test==0).sum():,} | Classe 1: {(y_test==1).sum():,}")
    
    # ### 6.3 Treinamento do Random Forest
    
    print("\n" + "=" * 80)
    print("üå≤ TREINAMENTO DO RANDOM FOREST")
    print("=" * 80)
    
    # Inicializar modelo Random Forest
    rf_model = RandomForestClassifier(
        n_estimators=100,
        max_depth=15,
        min_samples_split=10,
        min_samples_leaf=5,
        random_state=42,
        n_jobs=-1,
        verbose=1
    )
    
    print("\n‚è≥ Treinando Random Forest...")
    print(f"   Par√¢metros: n_estimators=100, max_depth=15")
    
    rf_model.fit(X_train, y_train)
    
    print("‚úÖ Modelo Random Forest treinado com sucesso!")
    
    # Predi√ß√µes
    y_pred_rf_train = rf_model.predict(X_train)
    y_pred_rf_test = rf_model.predict(X_test)
    y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]
    
    # M√©tricas de desempenho
    print(f"\nüìä DESEMPENHO DO RANDOM FOREST:")
    print(f"\n   Conjunto de Treino:")
    print(f"   ‚Ä¢ Acur√°cia:  {accuracy_score(y_train, y_pred_rf_train):.4f}")
    print(f"   ‚Ä¢ Precis√£o:  {precision_score(y_train, y_pred_rf_train):.4f}")
    print(f"   ‚Ä¢ Recall:    {recall_score(y_train, y_pred_rf_train):.4f}")
    print(f"   ‚Ä¢ F1-Score:  {f1_score(y_train, y_pred_rf_train):.4f}")
    
    print(f"\n   Conjunto de Teste:")
    acc_rf = accuracy_score(y_test, y_pred_rf_test)
    prec_rf = precision_score(y_test, y_pred_rf_test)
    rec_rf = recall_score(y_test, y_pred_rf_test)
    f1_rf = f1_score(y_test, y_pred_rf_test)
    auc_rf = roc_auc_score(y_test, y_pred_proba_rf)
    
    print(f"   ‚Ä¢ Acur√°cia:  {acc_rf:.4f}")
    print(f"   ‚Ä¢ Precis√£o:  {prec_rf:.4f}")
    print(f"   ‚Ä¢ Recall:    {rec_rf:.4f}")
    print(f"   ‚Ä¢ F1-Score:  {f1_rf:.4f}")
    print(f"   ‚Ä¢ AUC-ROC:   {auc_rf:.4f}")
    
    # Matriz de Confus√£o
    cm_rf = confusion_matrix(y_test, y_pred_rf_test)
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Matriz de confus√£o
    sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=axes[0],
                xticklabels=['Baixa/M√©dia', 'Alta'],
                yticklabels=['Baixa/M√©dia', 'Alta'])
    axes[0].set_title('Matriz de Confus√£o - Random Forest', fontsize=12, fontweight='bold')
    axes[0].set_ylabel('Valor Real')
    axes[0].set_xlabel('Valor Predito')
    
    # Curva ROC
    fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)
    axes[1].plot(fpr_rf, tpr_rf, color='darkorange', lw=2, 
                 label=f'ROC curve (AUC = {auc_rf:.3f})')
    axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Baseline')
    axes[1].set_xlim([0.0, 1.0])
    axes[1].set_ylim([0.0, 1.05])
    axes[1].set_xlabel('Taxa de Falsos Positivos')
    axes[1].set_ylabel('Taxa de Verdadeiros Positivos')
    axes[1].set_title('Curva ROC - Random Forest', fontsize=12, fontweight='bold')
    axes[1].legend(loc="lower right")
    axes[1].grid(alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # Import√¢ncia das Features
    feature_importance_rf = pd.DataFrame({
        'feature': features_ml,
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\nüîç IMPORT√ÇNCIA DAS FEATURES (Random Forest):")
    print(feature_importance_rf.to_string(index=False))
    
    # Visualizar import√¢ncia
    plt.figure(figsize=(10, 6))
    sns.barplot(data=feature_importance_rf.head(15), x='importance', y='feature', palette='viridis')
    plt.title('Top 15 Features Mais Importantes - Random Forest', fontsize=14, fontweight='bold')
    plt.xlabel('Import√¢ncia')
    plt.ylabel('Feature')
    plt.tight_layout()
    plt.show()
    
    # ### 6.4 Treinamento do XGBoost
    
    print("\n" + "=" * 80)
    print("üöÄ TREINAMENTO DO XGBOOST")
    print("=" * 80)
    
    # Calcular scale_pos_weight para balanceamento
    scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
    
    # Inicializar modelo XGBoost
    xgb_model = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        scale_pos_weight=scale_pos_weight,
        random_state=42,
        n_jobs=-1,
        eval_metric='logloss'
    )
    
    print("\n‚è≥ Treinando XGBoost...")
    print(f"   Par√¢metros: n_estimators=100, max_depth=6, learning_rate=0.1")
    print(f"   Scale pos weight: {scale_pos_weight:.2f}")
    
    xgb_model.fit(X_train, y_train, 
                  eval_set=[(X_test, y_test)],
                  verbose=False)
    
    print("‚úÖ Modelo XGBoost treinado com sucesso!")
    
    # Predi√ß√µes
    y_pred_xgb_train = xgb_model.predict(X_train)
    y_pred_xgb_test = xgb_model.predict(X_test)
    y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]
    
    # M√©tricas de desempenho
    print(f"\nüìä DESEMPENHO DO XGBOOST:")
    print(f"\n   Conjunto de Treino:")
    print(f"   ‚Ä¢ Acur√°cia:  {accuracy_score(y_train, y_pred_xgb_train):.4f}")
    print(f"   ‚Ä¢ Precis√£o:  {precision_score(y_train, y_pred_xgb_train):.4f}")
    print(f"   ‚Ä¢ Recall:    {recall_score(y_train, y_pred_xgb_train):.4f}")
    print(f"   ‚Ä¢ F1-Score:  {f1_score(y_train, y_pred_xgb_train):.4f}")
    
    print(f"\n   Conjunto de Teste:")
    acc_xgb = accuracy_score(y_test, y_pred_xgb_test)
    prec_xgb = precision_score(y_test, y_pred_xgb_test)
    rec_xgb = recall_score(y_test, y_pred_xgb_test)
    f1_xgb = f1_score(y_test, y_pred_xgb_test)
    auc_xgb = roc_auc_score(y_test, y_pred_proba_xgb)
    
    print(f"   ‚Ä¢ Acur√°cia:  {acc_xgb:.4f}")
    print(f"   ‚Ä¢ Precis√£o:  {prec_xgb:.4f}")
    print(f"   ‚Ä¢ Recall:    {rec_xgb:.4f}")
    print(f"   ‚Ä¢ F1-Score:  {f1_xgb:.4f}")
    print(f"   ‚Ä¢ AUC-ROC:   {auc_xgb:.4f}")
    
    # Matriz de Confus√£o
    cm_xgb = confusion_matrix(y_test, y_pred_xgb_test)
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Matriz de confus√£o
    sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Greens', ax=axes[0],
                xticklabels=['Baixa/M√©dia', 'Alta'],
                yticklabels=['Baixa/M√©dia', 'Alta'])
    axes[0].set_title('Matriz de Confus√£o - XGBoost', fontsize=12, fontweight='bold')
    axes[0].set_ylabel('Valor Real')
    axes[0].set_xlabel('Valor Predito')
    
    # Curva ROC
    fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_proba_xgb)
    axes[1].plot(fpr_xgb, tpr_xgb, color='green', lw=2, 
                 label=f'ROC curve (AUC = {auc_xgb:.3f})')
    axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Baseline')
    axes[1].set_xlim([0.0, 1.0])
    axes[1].set_ylim([0.0, 1.05])
    axes[1].set_xlabel('Taxa de Falsos Positivos')
    axes[1].set_ylabel('Taxa de Verdadeiros Positivos')
    axes[1].set_title('Curva ROC - XGBoost', fontsize=12, fontweight='bold')
    axes[1].legend(loc="lower right")
    axes[1].grid(alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # Import√¢ncia das Features
    feature_importance_xgb = pd.DataFrame({
        'feature': features_ml,
        'importance': xgb_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\nüîç IMPORT√ÇNCIA DAS FEATURES (XGBoost):")
    print(feature_importance_xgb.to_string(index=False))
    
    # Visualizar import√¢ncia
    plt.figure(figsize=(10, 6))
    sns.barplot(data=feature_importance_xgb.head(15), x='importance', y='feature', palette='viridis')
    plt.title('Top 15 Features Mais Importantes - XGBoost', fontsize=14, fontweight='bold')
    plt.xlabel('Import√¢ncia')
    plt.ylabel('Feature')
    plt.tight_layout()
    plt.show()
    
    # ### 6.5 Compara√ß√£o de Modelos
    
    print("\n" + "=" * 80)
    print("üìä COMPARA√á√ÉO ENTRE MODELOS")
    print("=" * 80)
    
    # Tabela comparativa
    comparacao = pd.DataFrame({
        'Modelo': ['Random Forest', 'XGBoost'],
        'Acur√°cia': [acc_rf, acc_xgb],
        'Precis√£o': [prec_rf, prec_xgb],
        'Recall': [rec_rf, rec_xgb],
        'F1-Score': [f1_rf, f1_xgb],
        'AUC-ROC': [auc_rf, auc_xgb]
    })
    
    print("\n" + comparacao.to_string(index=False))
    
    # Visualiza√ß√£o comparativa
    fig = make_subplots(
        rows=1, cols=2,
        subplot_titles=('Compara√ß√£o de M√©tricas', 'Curvas ROC'),
        specs=[[{'type': 'bar'}, {'type': 'scatter'}]]
    )
    
    # Gr√°fico de barras comparativo
    metricas = ['Acur√°cia', 'Precis√£o', 'Recall', 'F1-Score', 'AUC-ROC']
    for idx, modelo in enumerate(['Random Forest', 'XGBoost']):
        valores = comparacao[comparacao['Modelo'] == modelo][metricas].values[0]
        fig.add_trace(
            go.Bar(x=metricas, y=valores, name=modelo,
                   marker_color='darkorange' if idx == 0 else 'green'),
            row=1, col=1
        )
    
    # Curvas ROC sobrepostas
    fig.add_trace(
        go.Scatter(x=fpr_rf, y=tpr_rf, mode='lines', name=f'Random Forest (AUC={auc_rf:.3f})',
                   line=dict(color='darkorange', width=2)),
        row=1, col=2
    )
    fig.add_trace(
        go.Scatter(x=fpr_xgb, y=tpr_xgb, mode='lines', name=f'XGBoost (AUC={auc_xgb:.3f})',
                   line=dict(color='green', width=2)),
        row=1, col=2
    )
    fig.add_trace(
        go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Baseline',
                   line=dict(color='navy', width=2, dash='dash')),
        row=1, col=2
    )
    
    fig.update_xaxes(title_text="M√©tricas", row=1, col=1)
    fig.update_yaxes(title_text="Valor", row=1, col=1)
    fig.update_xaxes(title_text="Taxa de Falsos Positivos", row=1, col=2)
    fig.update_yaxes(title_text="Taxa de Verdadeiros Positivos", row=1, col=2)
    
    fig.update_layout(height=500, title_text="üèÜ Compara√ß√£o de Desempenho dos Modelos Preditivos",
                      showlegend=True, barmode='group')
    fig.show()
    
    # Determinar melhor modelo
    melhor_modelo_nome = comparacao.loc[comparacao['F1-Score'].idxmax(), 'Modelo']
    melhor_f1 = comparacao['F1-Score'].max()
    
    print(f"\nüèÜ MELHOR MODELO: {melhor_modelo_nome}")
    print(f"   F1-Score: {melhor_f1:.4f}")
    
    # ### 6.6 Valida√ß√£o Cruzada
    
    print("\n" + "=" * 80)
    print("üîÑ VALIDA√á√ÉO CRUZADA (K-Fold)")
    print("=" * 80)
    
    # Random Forest
    print("\n‚è≥ Executando valida√ß√£o cruzada para Random Forest...")
    cv_scores_rf = cross_val_score(rf_model, X, y, cv=5, scoring='f1', n_jobs=-1)
    print(f"   F1-Scores por fold: {cv_scores_rf}")
    print(f"   M√©dia: {cv_scores_rf.mean():.4f} (+/- {cv_scores_rf.std():.4f})")
    
    # XGBoost
    print("\n‚è≥ Executando valida√ß√£o cruzada para XGBoost...")
    cv_scores_xgb = cross_val_score(xgb_model, X, y, cv=5, scoring='f1', n_jobs=-1)
    print(f"   F1-Scores por fold: {cv_scores_xgb}")
    print(f"   M√©dia: {cv_scores_xgb.mean():.4f} (+/- {cv_scores_xgb.std():.4f})")
    
    # Visualiza√ß√£o dos resultados de CV
    cv_results = pd.DataFrame({
        'Fold': list(range(1, 6)) * 2,
        'F1-Score': list(cv_scores_rf) + list(cv_scores_xgb),
        'Modelo': ['Random Forest'] * 5 + ['XGBoost'] * 5
    })
    
    fig = px.box(cv_results, x='Modelo', y='F1-Score', color='Modelo',
                 title='Distribui√ß√£o dos F1-Scores na Valida√ß√£o Cruzada (5-Fold)',
                 color_discrete_map={'Random Forest': 'darkorange', 'XGBoost': 'green'})
    fig.update_layout(height=500)
    fig.show()
    
    print("\n‚úÖ Modelagem Preditiva conclu√≠da!")

else:
    print("\n‚ö†Ô∏è  Coluna de FRP n√£o encontrada. Pulando modelagem preditiva supervisionada.")

# ---

# ## 7Ô∏è‚É£ Etapa 6: An√°lise de Predi√ß√£o Espacial e Temporal

print("\n" + "=" * 80)
print("üó∫Ô∏è AN√ÅLISE DE PREDI√á√ÉO ESPACIAL E TEMPORAL")
print("=" * 80)

if col_frp in df_processed.columns and 'alta_intensidade' in df_ml.columns:
    
    # Adicionar predi√ß√µes ao dataframe original
    df_ml_clean['pred_rf'] = y_pred_rf_test
    df_ml_clean['pred_xgb'] = y_pred_xgb_test
    df_ml_clean['proba_rf'] = y_pred_proba_rf
    df_ml_clean['proba_xgb'] = y_pred_proba_xgb
    
    # An√°lise de erros por regi√£o
    if 'estado' in df_ml_clean.columns:
        print("\nüìä AN√ÅLISE DE ERROS POR ESTADO:")
        
        # Calcular m√©tricas por estado para o melhor modelo
        melhor_modelo_pred = 'pred_rf' if melhor_modelo_nome == 'Random Forest' else 'pred_xgb'
        
        estados_metricas = []
        for estado in df_ml_clean['estado'].unique()[:10]:  # Top 10 estados
            df_estado = df_ml_clean[df_ml_clean['estado'] == estado]
            if len(df_estado) > 50:  # M√≠nimo de amostras
                acc_estado = accuracy_score(df_estado['alta_intensidade'], 
                                            df_estado[melhor_modelo_pred])
                estados_metricas.append({
                    'Estado': estado,
                    'Amostras': len(df_estado),
                    'Acur√°cia': acc_estado
                })
        
        df_estados_metricas = pd.DataFrame(estados_metricas).sort_values('Acur√°cia', ascending=False)
        print(df_estados_metricas.to_string(index=False))
    
    # Mapa de predi√ß√µes
    if 'latitude' in df_ml_clean.columns and 'longitude' in df_ml_clean.columns:
        print("\nüó∫Ô∏è  Gerando mapa de predi√ß√µes...")
        
        # Amostrar para visualiza√ß√£o
        df_map_sample = df_ml_clean.sample(min(5000, len(df_ml_clean)))
        
        # Recuperar coordenadas originais
        lat_map = df_processed.loc[df_map_sample.index, 'latitude']
        lon_map = df_processed.loc[df_map_sample.index, 'longitude']
        
        # Classifica√ß√£o: Correto vs Incorreto
        melhor_pred = 'pred_rf' if melhor_modelo_nome == 'Random Forest' else 'pred_xgb'
        df_map_sample['classificacao'] = df_map_sample.apply(
            lambda row: 'Correto' if row['alta_intensidade'] == row[melhor_pred] else 'Incorreto',
            axis=1
        )
        
        fig = px.scatter_mapbox(
            df_map_sample,
            lat=lat_map,
            lon=lon_map,
            color='classificacao',
            color_discrete_map={'Correto': 'green', 'Incorreto': 'red'},
            zoom=4,
            height=600,
            title=f'üó∫Ô∏è Mapa de Predi√ß√µes - {melhor_modelo_nome} (Amostra)',
            labels={'classificacao': 'Classifica√ß√£o'},
            hover_data=['alta_intensidade', melhor_pred]
        )
        
        fig.update_layout(mapbox_style="open-street-map")
        fig.update_traces(marker=dict(size=5, opacity=0.6))
        fig.show()

# ---

# ## 8Ô∏è‚É£ Etapa 7: Insights e Recomenda√ß√µes

print("\n" + "=" * 80)
print("üí° INSIGHTS E RECOMENDA√á√ïES ESTRAT√âGICAS")
print("=" * 80)

print("\n" + "üî•" * 40)
print("PRINCIPAIS INSIGHTS EXTRA√çDOS DA AN√ÅLISE")
print("üî•" * 40)

# Insight 1: Padr√µes Temporais
print("\n1Ô∏è‚É£  PADR√ïES TEMPORAIS:")
if 'mes' in df.columns:
    mes_critico = ocorrencias_mensais.loc[ocorrencias_mensais['ocorrencias'].idxmax(), 'nome_mes']
    focos_mes_critico = ocorrencias_mensais['ocorrencias'].max()
    print(f"   ‚Ä¢ M√™s mais cr√≠tico: {mes_critico} com {focos_mes_critico:,} focos")
    print(f"   ‚Ä¢ Per√≠odo de seca (jul-out) concentra {df[df['periodo_critico']==1].shape[0]/len(df)*100:.1f}% dos focos")
    print(f"   ‚Ä¢ Recomenda√ß√£o: Intensificar monitoramento e recursos nos meses de julho a outubro")

# Insight 2: Distribui√ß√£o Espacial
print("\n2Ô∏è‚É£  DISTRIBUI√á√ÉO ESPACIAL:")
if 'estado' in df.columns:
    estado_critico = ocorrencias_estado.iloc[0]['estado']
    focos_estado = ocorrencias_estado.iloc[0]['ocorrencias']
    print(f"   ‚Ä¢ Estado mais afetado: {estado_critico} ({focos_estado:,} focos)")
    print(f"   ‚Ä¢ Top 3 estados concentram {ocorrencias_estado.head(3)['ocorrencias'].sum()/len(df)*100:.1f}% dos focos")
    print(f"   ‚Ä¢ Recomenda√ß√£o: Criar centros regionais de resposta r√°pida nos estados mais afetados")

# Insight 3: Clusters Identificados
print("\n3Ô∏è‚É£  PADR√ïES DE AGRUPAMENTO:")
if 'cluster' in df_processed.columns:
    n_clusters_final = df_processed['cluster'].nunique()
    print(f"   ‚Ä¢ {n_clusters_final} clusters distintos identificados com caracter√≠sticas √∫nicas")
    print(f"   ‚Ä¢ Clusters revelam padr√µes espaciais, temporais e de intensidade")
    print(f"   ‚Ä¢ Recomenda√ß√£o: Desenvolver estrat√©gias espec√≠ficas para cada tipo de cluster")

# Insight 4: Vari√°veis Preditivas
print("\n4Ô∏è‚É£  FATORES PREDITIVOS:")
if 'alta_intensidade' in locals() and melhor_modelo_nome:
    top_features = feature_importance_rf if melhor_modelo_nome == 'Random Forest' else feature_importance_xgb
    print(f"   ‚Ä¢ Features mais importantes para predi√ß√£o:")
    for idx, row in top_features.head(5).iterrows():
        print(f"      {idx+1}. {row['feature']}: {row['importance']:.4f}")
    print(f"   ‚Ä¢ Modelo {melhor_modelo_nome} atingiu F1-Score de {melhor_f1:.4f}")
    print(f"   ‚Ä¢ Recomenda√ß√£o: Monitorar ativamente as vari√°veis de maior import√¢ncia preditiva")

# Insight 5: Intensidade dos Focos
print("\n5Ô∏è‚É£  INTENSIDADE DAS QUEIMADAS:")
if col_frp in df.columns:
    frp_medio = df[col_frp].mean()
    frp_max = df[col_frp].max()
    focos_alta_intensidade = (df[col_frp] > limiar_intensidade).sum()
    print(f"   ‚Ä¢ Intensidade m√©dia (FRP): {frp_medio:.2f} MW")
    print(f"   ‚Ä¢ Intensidade m√°xima registrada: {frp_max:.2f} MW")
    print(f"   ‚Ä¢ {focos_alta_intensidade:,} focos classificados como alta intensidade")
    print(f"   ‚Ä¢ Recomenda√ß√£o: Priorizar combate a focos com FRP > {limiar_intensidade:.1f} MW")

print("\n" + "=" * 80)
print("üéØ RECOMENDA√á√ïES ESTRAT√âGICAS PARA GEST√ÉO E PREVEN√á√ÉO")
print("=" * 80)

recomendacoes = [
    {
        'titulo': 'üö® Sistema de Alerta Precoce',
        'descricao': 'Implementar sistema automatizado de alertas baseado nos modelos preditivos desenvolvidos, '
                     'com limiares de probabilidade calibrados para diferentes n√≠veis de a√ß√£o (amarelo, laranja, vermelho).',
        'prazo': 'Curto prazo (3-6 meses)',
        'impacto': 'Alto'
    },
    {
        'titulo': 'üó∫Ô∏è Mapeamento de √Åreas Priorit√°rias',
        'descricao': 'Criar mapas de risco atualizados mensalmente identificando hotspots cr√≠ticos para '
                     'aloca√ß√£o otimizada de equipes de monitoramento e combate.',
        'prazo': 'Imediato (1-3 meses)',
        'impacto': 'Alto'
    },
    {
        'titulo': 'üìä Dashboard de Monitoramento em Tempo Real',
        'descricao': 'Desenvolver plataforma web interativa consolidando dados de sat√©lite, previs√µes dos modelos, '
                     'alertas ativos e status de recursos de combate, acess√≠vel para gestores e brigadas.',
        'prazo': 'M√©dio prazo (6-12 meses)',
        'impacto': 'Muito Alto'
    },
    {
        'titulo': 'üë• Capacita√ß√£o de Equipes Locais',
        'descricao': 'Treinar comunidades locais, fazendeiros e brigadistas em identifica√ß√£o precoce de focos, '
                     'uso de aplicativos de reporte, e t√©cnicas de preven√ß√£o baseadas nos padr√µes identificados.',
        'prazo': 'Cont√≠nuo',
        'impacto': 'M√©dio-Alto'
    },
    {
        'titulo': 'üå± Pol√≠ticas de Uso Sustent√°vel do Solo',
        'descricao': 'Estabelecer regulamenta√ß√µes mais rigorosas para queimadas controladas nos per√≠odos cr√≠ticos, '
                     'incentivando pr√°ticas agr√≠colas alternativas ao uso do fogo.',
        'prazo': 'Longo prazo (1-2 anos)',
        'impacto': 'Alto'
    },
    {
        'titulo': 'üõ∞Ô∏è Integra√ß√£o com Dados Meteorol√≥gicos',
        'descricao': 'Incorporar vari√°veis meteorol√≥gicas de alta resolu√ß√£o (temperatura, umidade, vento) aos '
                     'modelos preditivos para aumentar acur√°cia e antecipa√ß√£o de eventos cr√≠ticos.',
        'prazo': 'M√©dio prazo (6-12 meses)',
        'impacto': 'Alto'
    },
    {
        'titulo': 'ü§ù Parcerias Interinstitucionais',
        'descricao': 'Estabelecer coopera√ß√£o entre INPE, IBAMA, ICMBio, Corpo de Bombeiros, universidades e ONGs '
                     'para compartilhamento de dados, recursos e expertise t√©cnica.',
        'prazo': 'Curto prazo (3-6 meses)',
        'impacto': 'M√©dio'
    }
]

for idx, rec in enumerate(recomendacoes, 1):
    print(f"\n{idx}. {rec['titulo']}")
    print(f"   üìã Descri√ß√£o: {rec['descricao']}")
    print(f"   ‚è∞ Prazo: {rec['prazo']}")
    print(f"   üí• Impacto Esperado: {rec['impacto']}")

# ---

# ## 9Ô∏è‚É£ Conclus√µes e Trabalhos Futuros

print("\n" + "=" * 80)
print("üìù CONCLUS√ïES E TRABALHOS FUTUROS")
print("=" * 80)

print("\n" + "‚úÖ" * 40)
print("CONCLUS√ïES PRINCIPAIS")
print("‚úÖ" * 40)

print("""
Este projeto desenvolveu um sistema inteligente abrangente para an√°lise, monitoramento e predi√ß√£o 
de queimadas no Pantanal utilizando t√©cnicas avan√ßadas de Ci√™ncia de Dados e Aprendizado de M√°quina 
aplicadas a dados geoespaciais reais de 2020.

üéØ PRINCIPAIS CONQUISTAS:

1. An√°lise Explorat√≥ria Robusta:
   ‚Ä¢ Processamento e an√°lise de milhares de registros de focos de queimadas
   ‚Ä¢ Identifica√ß√£o de padr√µes temporais, espaciais e de intensidade
   ‚Ä¢ Caracteriza√ß√£o detalhada da crise de queimadas de 2020 no Pantanal

2. Modelagem N√£o Supervisionada:
   ‚Ä¢ Aplica√ß√£o bem-sucedida de K-Means e DBSCAN para identifica√ß√£o de clusters
   ‚Ä¢ Descoberta de agrupamentos naturais com caracter√≠sticas distintas
   ‚Ä¢ Valida√ß√£o atrav√©s de m√∫ltiplas m√©tricas (silhueta, Davies-Bouldin, Calinski-Harabasz)

3. Modelagem Preditiva:
   ‚Ä¢ Desenvolvimento de modelos Random Forest e XGBoost com alta acur√°cia
   ‚Ä¢ Identifica√ß√£o das vari√°veis mais importantes para predi√ß√£o
   ‚Ä¢ Valida√ß√£o rigorosa atrav√©s de valida√ß√£o cruzada e m√©tricas m√∫ltiplas

4. Visualiza√ß√µes Informativas:
   ‚Ä¢ Mapas interativos mostrando distribui√ß√£o espacial dos focos
   ‚Ä¢ S√©ries temporais revelando sazonalidade e tend√™ncias
   ‚Ä¢ Dashboards integrados facilitando interpreta√ß√£o dos resultados

5. Insights Acion√°veis:
   ‚Ä¢ Identifica√ß√£o de per√≠odos e regi√µes cr√≠ticas para intensifica√ß√£o do monitoramento
   ‚Ä¢ Recomenda√ß√µes concretas para pol√≠ticas p√∫blicas e estrat√©gias de preven√ß√£o
   ‚Ä¢ Base cient√≠fica s√≥lida para tomada de decis√£o em gest√£o ambiental

üìä CONTRIBUI√á√ïES CIENT√çFICAS:

‚Ä¢ Demonstra√ß√£o da aplicabilidade de t√©cnicas de ML em problemas ambientais complexos
‚Ä¢ Metodologia replic√°vel para an√°lise de queimadas em outros biomas brasileiros
‚Ä¢ Integra√ß√£o efetiva entre an√°lise explorat√≥ria, modelagem e visualiza√ß√£o geoespacial
‚Ä¢ Framework para desenvolvimento de sistemas operacionais de alerta precoce

‚ö†Ô∏è LIMITA√á√ïES IDENTIFICADAS:

‚Ä¢ Dados limitados a um √∫nico ano (2020) - necessidade de s√©ries hist√≥ricas mais longas
‚Ä¢ Aus√™ncia de vari√°veis meteorol√≥gicas detalhadas que poderiam melhorar predi√ß√µes
‚Ä¢ Poss√≠veis vieses de detec√ß√£o por sat√©lite (cobertura de nuvens, resolu√ß√£o temporal)
‚Ä¢ Modelos n√£o consideram explicitamente autocorrela√ß√£o espacial dos dados

üîÆ TRABALHOS FUTUROS RECOMENDADOS:

1. Expans√£o Temporal:
   ‚Ä¢ Incorporar dados de m√∫ltiplos anos (2015-2024) para an√°lise de tend√™ncias de longo prazo
   ‚Ä¢ Desenvolver modelos de s√©ries temporais (ARIMA, LSTM) para predi√ß√£o sequencial

2. Enriquecimento de Dados:
   ‚Ä¢ Integrar vari√°veis meteorol√≥gicas (temperatura, umidade, precipita√ß√£o, vento)
   ‚Ä¢ Adicionar dados de uso do solo, proximidade a √°reas urbanas e estradas
   ‚Ä¢ Incorporar √≠ndices de vegeta√ß√£o derivados de imagens de sat√©lite (NDVI, EVI)

3. Modelagem Avan√ßada:
   ‚Ä¢ Implementar modelos espacialmente expl√≠citos (Spatial Random Forest, GWR)
   ‚Ä¢ Desenvolver redes neurais deep learning (CNN para imagens, LSTM para s√©ries temporais)
   ‚Ä¢ Explorar ensemble methods combinando m√∫ltiplos algoritmos

4. Sistema Operacional:
   ‚Ä¢ Desenvolver API para consumo de predi√ß√µes em tempo real
   ‚Ä¢ Criar aplicativo m√≥vel para brigadistas e comunidades locais
   ‚Ä¢ Implementar pipeline automatizado de atualiza√ß√£o de dados e retreinamento de modelos

5. An√°lise de Impactos:
   ‚Ä¢ Avaliar danos ambientais e socioecon√¥micos associados √†s queimadas
   ‚Ä¢ Estimar emiss√µes de carbono e impactos clim√°ticos
   ‚Ä¢ Analisar efeitos na biodiversidade e em esp√©cies amea√ßadas

6. Valida√ß√£o de Campo:
   ‚Ä¢ Realizar valida√ß√µes in loco das predi√ß√µes dos modelos
   ‚Ä¢ Coletar dados de campo para calibra√ß√£o e melhoria dos algoritmos
   ‚Ä¢ Estabelecer parceria com brigadas para feedback sobre utilidade operacional

""")

print("=" * 80)
print("üèÜ PROJETO CONCLU√çDO COM SUCESSO!")
print("=" * 80)
print(f"""
üìä Estat√≠sticas Finais do Projeto:
   ‚Ä¢ Dataset analisado: {len(df):,} registros de focos de queimadas
   ‚Ä¢ Per√≠odo: {df['data'].min()} a {df['data'].max() if 'data' in df.columns else 'N/A'}
   ‚Ä¢ Features criadas: {len([c for c in df_processed.columns if c not in df_original.columns])}
   ‚Ä¢ Modelos desenvolvidos: 2+ (K-Means, Random Forest, XGBoost)
   ‚Ä¢ Visualiza√ß√µes geradas: 15+
   ‚Ä¢ Insights extra√≠dos: 5 principais
   ‚Ä¢ Recomenda√ß√µes estrat√©gicas: 7

üåø Contribui√ß√£o para Conserva√ß√£o do Pantanal:
Este projeto fornece ferramentas cient√≠ficas robustas e acion√°veis para apoiar a 
conserva√ß√£o de um dos biomas mais ricos e amea√ßados do planeta, contribuindo para
a prote√ß√£o da biodiversidade, sustentabilidade ambiental e bem-estar das comunidades
que dependem deste ecossistema √∫nico.

‚ú® Ci√™ncia de Dados a Servi√ßo da Preserva√ß√£o Ambiental ‚ú®
""")

print("\nüéì Projeto desenvolvido para a disciplina de Aplica√ß√µes em Aprendizado de M√°quina")
print("üè´ Curso: Ci√™ncia de Dados")
print("üìÖ Ano: 2025")
print("\n" + "üî•" * 40)
print("OBRIGADO!")
print("üî•" * 40)

# FIM DO NOTEBOOK 
